{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "data_str = [\"hello spark\", \"hello lover\", \"hello world\"]\n",
    "data_num = [1,2,3,4,5]\n",
    "\n",
    "\n",
    "# def my_map():\n",
    "rdd1 = sc.parallelize(data_num)\n",
    "rdd2 = rdd1.map(lambda x:x+1)\n",
    "print(rdd2.collect())\n",
    "# [2, 3, 4, 5, 6]\n",
    "\n",
    "# def my_map2():\n",
    "rdd1 = sc.parallelize(data_str)\n",
    "rdd2 = rdd1.map(lambda x:(x,1))\n",
    "print(rdd2.collect())\n",
    "# [('hello spark', 1), ('hello lover', 1), ('hello world', 1)]\n",
    "\n",
    "\n",
    "# def my_filter():\n",
    "rdd1 = sc.parallelize(data_num)\n",
    "rdd2 = rdd1.filter(lambda x: x > 3)\n",
    "print(rdd2.collect())\n",
    "# [4, 5]\n",
    "\n",
    "\n",
    "# def my_flatMap():\n",
    "rdd1 = sc.parallelize(data_str)\n",
    "rdd2 = rdd1.flatMap(lambda line: line.split(\" \"))\n",
    "print(rdd2.collect())\n",
    "# ['hello', 'spark', 'hello', 'lover', 'hello', 'world']\n",
    "\n",
    "\n",
    "# def my_groupBy():\n",
    "rdd1 = sc.parallelize(data_str)\n",
    "rdd2 = rdd1.flatMap(lambda x: x.split(\" \")).map(lambda x: (x, 1))\n",
    "rdd3 = rdd2.groupByKey()\n",
    "print(rdd3.collect())\n",
    "# [('world', <pyspark.resultiterable.ResultIterable object at 0x1133efeb8>), ('hello', <pyspark.resultiterable.ResultIterable object at 0x113407438>), ('spark', <pyspark.resultiterable.ResultIterable object at 0x113407b70>), ('lover', <pyspark.resultiterable.ResultIterable object at 0x113407780>)]\n",
    "print(rdd3.map(lambda x: {x[0]: list(x[1])}).collect())\n",
    "# [{'world': [1]}, {'hello': [1, 1, 1]}, {'spark': [1]}, {'lover': [1]}]\n",
    "\n",
    "\n",
    "# def my_reduceByKey():\n",
    "rdd1 = sc.parallelize(data_str)\n",
    "rdd2 = rdd1.flatMap(lambda x: x.split(\" \")).map(lambda x: (x, 1))\n",
    "rdd3 = rdd2.reduceByKey(lambda a, b: a + b)\n",
    "print(rdd3.collect())\n",
    "# [('world', 1), ('hello', 3), ('spark', 1), ('lover', 1)]\n",
    "\n",
    "\n",
    "# def my_sort():\n",
    "rdd1 = sc.parallelize(data_str)\n",
    "rdd2 = rdd1.flatMap(lambda x: x.split(\" \")).map(lambda x: (x, 1))\n",
    "rdd3 = rdd2.reduceByKey(lambda a, b: a + b)\n",
    "# rdd4 = rdd3.sortByKey().collect()\n",
    "# [('world', 1), ('hello', 3), ('spark', 1), ('lover', 1)]\n",
    "rdd4 = rdd3.map(lambda x:(x[1], x[0])).sortByKey()\n",
    "print(rdd4.collect())\n",
    "# [(1, 'world'), (1, 'spark'), (1, 'lover'), (3, 'hello')]\n",
    "rdd4 = rdd3.map(lambda x: (x[1], x[0])).sortByKey().map(lambda x: (x[1], x[0]))\n",
    "print(rdd4.collect())\n",
    "# [('world', 1), ('spark', 1), ('lover', 1), ('hello', 3)]\n",
    "\n",
    "\n",
    "# def my_union():\n",
    "a = sc.parallelize([1,2,3])\n",
    "b = sc.parallelize([3,4,5])\n",
    "print(a.union(b).collect())\n",
    "# [1, 2, 3, 3, 4, 5]\n",
    "\n",
    "\n",
    "# def my_distinct():\n",
    "a = sc.parallelize([1, 2, 3])\n",
    "b = sc.parallelize([2, 3, 4])\n",
    "print(a.union(b).distinct().collect())\n",
    "# [4, 1, 2, 3]\n",
    "\n",
    "# def my_join():\n",
    "a = sc.parallelize([('A','a1'),('C','c1'),('D','d1'),('F','f1'),('F','f2')])\n",
    "b = sc.parallelize([('A','a2'),('C','c2'),('C','c3'),('E','e1')])\n",
    "print(a.join(b).collect())\n",
    "# 内连接，以共有的元素为准，作一次二元组合\n",
    "# [('A', ('a1', 'a2')), ('C', ('c1', 'c2')), ('C', ('c1', 'c3'))]\n",
    "print(a.leftOuterJoin(b).collect())\n",
    "# 左连接，以左边的表为准，作一次二元组合，若右边的表没有则补上 None\n",
    "# [('A', ('a1', 'a2')), ('F', ('f1', None)), ('F', ('f2', None)), ('C', ('c1', 'c2')), ('C', ('c1', 'c3')), ('D', ('d1', None))]\n",
    "print(a.rightOuterJoin(b).collect())\n",
    "# 右连接，以右边的表为准，作一次二元组合，若左边的表没有则补上 None左\n",
    "# [('A', ('a1', 'a2')), ('C', ('c1', 'c2')), ('C', ('c1', 'c3')), ('E', (None, 'e1'))]\n",
    "print(a.fullOuterJoin(b).collect())\n",
    "# 全连接，用两边全部元素，作二元组合，任一边没有的则补上 None\n",
    "# [('A', ('a1', 'a2')), ('F', ('f1', None)), ('F', ('f2', None)), ('C', ('c1', 'c2')), ('C', ('c1', 'c3')), ('D', ('d1', None)), ('E', (None, 'e1'))]\n",
    "\n",
    "\n",
    "# def my_action():\n",
    "rdd = sc.parallelize(data_num)\n",
    "rdd.count()\n",
    "rdd.take(3)\n",
    "rdd.max()\n",
    "rdd.min()\n",
    "rdd.sum()\n",
    "print()\n",
    "print(rdd.reduce(lambda x,y:x+y))\n",
    "# 15\n",
    "print(rdd.foreach(lambda x:print(x)))\n",
    "# 1\n",
    "# 2\n",
    "# 3\n",
    "# 4\n",
    "# 5\n",
    "# None\n",
    "\n",
    "\n",
    "\n",
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
