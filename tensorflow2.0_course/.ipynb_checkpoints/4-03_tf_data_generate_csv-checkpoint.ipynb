{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 400
    },
    "colab_type": "code",
    "id": "AF-4sRdM5zl-",
    "outputId": "ff9cd523-223e-47a2-f242-aa0d5169c1dc",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow==2.0.0rc in /usr/local/lib/python3.6/dist-packages (2.0.0rc0)\n",
      "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0rc) (0.2.2)\n",
      "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0rc) (0.1.7)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0rc) (0.8.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0rc) (1.1.0)\n",
      "Requirement already satisfied: tf-estimator-nightly<1.14.0.dev2019080602,>=1.14.0.dev2019080601 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0rc) (1.14.0.dev2019080601)\n",
      "Requirement already satisfied: tb-nightly<1.15.0a20190807,>=1.15.0a20190806 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0rc) (1.15.0a20190806)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0rc) (3.0.1)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0rc) (1.11.2)\n",
      "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0rc) (1.16.5)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0rc) (1.15.0)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0rc) (3.7.1)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0rc) (1.1.0)\n",
      "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0rc) (1.0.8)\n",
      "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0rc) (0.8.0)\n",
      "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0rc) (1.12.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0rc) (0.33.6)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.15.0a20190807,>=1.15.0a20190806->tensorflow==2.0.0rc) (41.2.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.15.0a20190807,>=1.15.0a20190806->tensorflow==2.0.0rc) (3.1.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.15.0a20190807,>=1.15.0a20190806->tensorflow==2.0.0rc) (0.15.6)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow==2.0.0rc) (2.8.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow==2.0.0rc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 155
    },
    "colab_type": "code",
    "id": "rGJrXCer48U3",
    "outputId": "13338412-6d6f-41b9-a61c-9a8d4f77fe11",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0-rc0\n",
      "sys.version_info(major=3, minor=6, micro=3, releaselevel='final', serial=0)\n",
      "matplotlib 3.1.1\n",
      "numpy 1.17.1\n",
      "pandas 0.25.1\n",
      "sklearn 0.21.3\n",
      "tensorflow 2.0.0-rc0\n",
      "tensorflow.python.keras.api._v2.keras 2.2.4-tf\n"
     ]
    }
   ],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import pprint\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "print(tf.__version__)\n",
    "print(sys.version_info)\n",
    "for module in mpl, np, pd, sklearn, tf, keras:\n",
    "    print(module.__name__, module.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "colab_type": "code",
    "id": "4aSirXUOafmH",
    "outputId": "536ad686-4680-4a23-e086-25dd5cba35ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: Not connected to a TPU runtime; please see the first cell in this notebook for instructions!\n"
     ]
    }
   ],
   "source": [
    "if 'COLAB_TPU_ADDR' not in os.environ:\n",
    "  print('ERROR: Not connected to a TPU runtime; please see the first cell in this notebook for instructions!')\n",
    "else:\n",
    "  tpu_address = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
    "  print ('TPU address is', tpu_address)\n",
    "\n",
    "  with tf.Session(tpu_address) as session:\n",
    "    devices = session.list_devices()\n",
    "    \n",
    "  print('TPU devices:')\n",
    "  pprint.pprint(devices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 849
    },
    "colab_type": "code",
    "id": "Sztw6V1w5IEW",
    "outputId": "b0a3d79f-0f2d-43ca-eaa5-00bc6981b094"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('.. _california_housing_dataset:\\n'\n",
      " '\\n'\n",
      " 'California Housing dataset\\n'\n",
      " '--------------------------\\n'\n",
      " '\\n'\n",
      " '**Data Set Characteristics:**\\n'\n",
      " '\\n'\n",
      " '    :Number of Instances: 20640\\n'\n",
      " '\\n'\n",
      " '    :Number of Attributes: 8 numeric, predictive attributes and the target\\n'\n",
      " '\\n'\n",
      " '    :Attribute Information:\\n'\n",
      " '        - MedInc        median income in block\\n'\n",
      " '        - HouseAge      median house age in block\\n'\n",
      " '        - AveRooms      average number of rooms\\n'\n",
      " '        - AveBedrms     average number of bedrooms\\n'\n",
      " '        - Population    block population\\n'\n",
      " '        - AveOccup      average house occupancy\\n'\n",
      " '        - Latitude      house block latitude\\n'\n",
      " '        - Longitude     house block longitude\\n'\n",
      " '\\n'\n",
      " '    :Missing Attribute Values: None\\n'\n",
      " '\\n'\n",
      " 'This dataset was obtained from the StatLib repository.\\n'\n",
      " 'http://lib.stat.cmu.edu/datasets/\\n'\n",
      " '\\n'\n",
      " 'The target variable is the median house value for California districts.\\n'\n",
      " '\\n'\n",
      " 'This dataset was derived from the 1990 U.S. census, using one row per '\n",
      " 'census\\n'\n",
      " 'block group. A block group is the smallest geographical unit for which the '\n",
      " 'U.S.\\n'\n",
      " 'Census Bureau publishes sample data (a block group typically has a '\n",
      " 'population\\n'\n",
      " 'of 600 to 3,000 people).\\n'\n",
      " '\\n'\n",
      " 'It can be downloaded/loaded using the\\n'\n",
      " ':func:`sklearn.datasets.fetch_california_housing` function.\\n'\n",
      " '\\n'\n",
      " '.. topic:: References\\n'\n",
      " '\\n'\n",
      " '    - Pace, R. Kelley and Ronald Barry, Sparse Spatial Autoregressions,\\n'\n",
      " '      Statistics and Probability Letters, 33 (1997) 291-297\\n')\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "array([ 8.30140000e+00,  2.10000000e+01,  6.23813708e+00,  9.71880492e-01,\n",
      "        2.40100000e+03,  2.10984183e+00,  3.78600000e+01, -1.22220000e+02])\n",
      "array([4.526, 3.585, 3.521, ..., 0.923, 0.847, 0.894])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "pprint.pprint(housing.DESCR)\n",
    "print('>'*90)\n",
    "pprint.pprint(housing.data[1])\n",
    "pprint.pprint(housing.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "0fMjqmhw6Zts",
    "outputId": "1abce737-5c9e-4860-9602-0db7189e57e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11610, 8) (11610,)\n",
      "(3870, 8) (3870,)\n",
      "(5160, 8) (5160,)\n"
     ]
    }
   ],
   "source": [
    "# 由上面可以知道，有 8 个特征（8 列）,1 个输出（target），20640 个样本（20640 行），\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 切分训练集与测试集\n",
    "x_train_all, x_test, y_train_all, y_test = train_test_split(\n",
    "    housing.data, housing.target, random_state = 7\n",
    ")\n",
    "\n",
    "# 切分全部训练集为训练集与验证集\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(\n",
    "    x_train_all, y_train_all, random_state = 11\n",
    ")\n",
    "\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_valid.shape, y_valid.shape)\n",
    "print(x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cHEnD5QI9eop"
   },
   "outputs": [],
   "source": [
    "# 数据预处理---标准化（对同一列：减均值，除方差）\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# 派出对象去处理嘛\n",
    "scaler = StandardScaler()\n",
    "# 分别标准化三部分输入\n",
    "x_train_scaled = scaler.fit_transform(x_train)\n",
    "x_valid_scaled = scaler.transform(x_valid)\n",
    "x_test_scaled = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 760
    },
    "colab_type": "code",
    "id": "71wJjqXF_QLk",
    "outputId": "aa2883e7-7862-469d-d2d3-432549090f6f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train filenames:\n",
      "['generate_csv/train_00.csv',\n",
      " 'generate_csv/train_01.csv',\n",
      " 'generate_csv/train_02.csv',\n",
      " 'generate_csv/train_03.csv',\n",
      " 'generate_csv/train_04.csv',\n",
      " 'generate_csv/train_05.csv',\n",
      " 'generate_csv/train_06.csv',\n",
      " 'generate_csv/train_07.csv',\n",
      " 'generate_csv/train_08.csv',\n",
      " 'generate_csv/train_09.csv',\n",
      " 'generate_csv/train_10.csv',\n",
      " 'generate_csv/train_11.csv',\n",
      " 'generate_csv/train_12.csv',\n",
      " 'generate_csv/train_13.csv',\n",
      " 'generate_csv/train_14.csv',\n",
      " 'generate_csv/train_15.csv',\n",
      " 'generate_csv/train_16.csv',\n",
      " 'generate_csv/train_17.csv',\n",
      " 'generate_csv/train_18.csv',\n",
      " 'generate_csv/train_19.csv']\n",
      "valid filenames:\n",
      "['generate_csv/valid_00.csv',\n",
      " 'generate_csv/valid_01.csv',\n",
      " 'generate_csv/valid_02.csv',\n",
      " 'generate_csv/valid_03.csv',\n",
      " 'generate_csv/valid_04.csv',\n",
      " 'generate_csv/valid_05.csv',\n",
      " 'generate_csv/valid_06.csv',\n",
      " 'generate_csv/valid_07.csv',\n",
      " 'generate_csv/valid_08.csv',\n",
      " 'generate_csv/valid_09.csv']\n",
      "test filenames:\n",
      "['generate_csv/test_00.csv',\n",
      " 'generate_csv/test_01.csv',\n",
      " 'generate_csv/test_02.csv',\n",
      " 'generate_csv/test_03.csv',\n",
      " 'generate_csv/test_04.csv',\n",
      " 'generate_csv/test_05.csv',\n",
      " 'generate_csv/test_06.csv',\n",
      " 'generate_csv/test_07.csv',\n",
      " 'generate_csv/test_08.csv',\n",
      " 'generate_csv/test_09.csv']\n"
     ]
    }
   ],
   "source": [
    "# ed\n",
    "\n",
    "output_dir = \"generate_csv\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.mkdir(output_dir)\n",
    "\n",
    "# 定义持久化函数（使用 csv 格式保存至指定文件夹output_dir）\n",
    "def save_to_csv(output_dir, data, name_prefix, header=None, n_parts=10):\n",
    "    path_format = os.path.join(output_dir, \"{}_{:02d}.csv\")\n",
    "    filenames = []\n",
    "\n",
    "    for file_index, row_indices in enumerate(np.array_split(np.arange(len(data)), n_parts)):  \n",
    "        # .array_split --- Split an array into multiple sub-arrays.\n",
    "        part_csv = path_format.format(name_prefix, file_index)\n",
    "        filenames.append(part_csv)\n",
    "\n",
    "        with open(part_csv, \"wt\", encoding=\"utf-8\") as f:\n",
    "            if header is not None:\n",
    "                f.write(header + '\\n')\n",
    "            for row_index in row_indices:\n",
    "                f.write(\",\".join([repr(col) for col in data[row_index]]))\n",
    "                # repr() --- Return the canonical string representation of the object.\n",
    "                f.write('\\n')  #不要文件在 csv 文件中加入换行符\n",
    "    return filenames\n",
    "\n",
    "train_data = np.c_[x_train_scaled, y_train]\n",
    "valid_data = np.c_[x_valid_scaled, y_valid]\n",
    "test_data = np.c_[x_test_scaled, y_test]\n",
    "# np.c_  --- 按列连接两个矩阵，就是把两矩阵左右相加，要求行数相等，类似于pandas中的merge()\n",
    "# np.r_  --- 按行连接两个矩阵，就是把两矩阵上下相加，要求列数相等，类似于pandas中的concat()\n",
    "\n",
    "header_cols = housing.feature_names + [\"MidianHouseValue\"]\n",
    "header_str = \",\".join(header_cols)\n",
    "\n",
    "train_filenames = save_to_csv(output_dir, train_data, \"train\", header_str, n_parts=20)\n",
    "print(\"train filenames:\")\n",
    "pprint.pprint(train_filenames)\n",
    "\n",
    "valid_filenames = save_to_csv(output_dir, valid_data, \"valid\", header_str, n_parts=10)\n",
    "print(\"valid filenames:\")\n",
    "pprint.pprint(valid_filenames)\n",
    "\n",
    "test_filenames = save_to_csv(output_dir, test_data, \"test\",  header_str, n_parts=10)\n",
    "print(\"test filenames:\")\n",
    "pprint.pprint(test_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a9wHCZgfHDcD"
   },
   "outputs": [],
   "source": [
    "# ed\n",
    "\n",
    "# 从文件名读取csv 文件，并解析：步骤如下\n",
    "# 1. filename -> dataset\n",
    "# 2. read file -> dataset -> datasets -> merge\n",
    "# 3. parse csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "colab_type": "code",
    "id": "n-nZlArSHaZw",
    "outputId": "494ef548-a7b8-483f-a172-f865a85e8681"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'generate_csv/train_00.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generate_csv/train_11.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generate_csv/train_06.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generate_csv/train_18.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generate_csv/train_17.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generate_csv/train_13.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generate_csv/train_16.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generate_csv/train_08.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generate_csv/train_15.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generate_csv/train_07.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generate_csv/train_09.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generate_csv/train_03.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generate_csv/train_12.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generate_csv/train_14.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generate_csv/train_05.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generate_csv/train_01.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generate_csv/train_02.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generate_csv/train_19.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generate_csv/train_04.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generate_csv/train_10.csv', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "# 以训练集的文件名为例\n",
    "filename_dataset = tf.data.Dataset.list_files(train_filenames)\n",
    "for filename in filename_dataset:\n",
    "    print(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 210
    },
    "colab_type": "code",
    "id": "65qq2rjCHnPi",
    "outputId": "ccca7310-b3fe-4007-8e11-4cda848cc350"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'0.8115083791797953,-0.04823952235146133,0.5187339067174729,-0.029386394873127775,-0.034064024638222286,-0.05081594842905086,-0.7157356834231196,0.9162751241885168,2.147'\n",
      "b'2.51504373119231,1.0731637904355105,0.5574401201546321,-0.17273513019187772,-0.612912610473286,-0.01909156503651574,-0.5710993036045546,-0.027490309606616956,5.00001'\n",
      "b'0.6303435674178064,1.874166156711919,-0.06713214279531016,-0.12543366804152128,-0.19737553788322462,-0.022722631725889016,-0.692407235065288,0.7265233438487496,2.419'\n",
      "b'0.04326300977263167,-1.0895425985107923,-0.38878716774583305,-0.10789864528874438,-0.6818663605100649,-0.0723871014747467,-0.8883662012710817,0.8213992340186296,1.426'\n",
      "b'0.04971034572063198,-0.8492418886278699,-0.06214699417830008,0.17878747064657746,-0.8025354230744277,0.0005066066922077538,0.6466457006743215,-1.1060793768010604,2.286'\n",
      "b'-0.6906143291679195,-0.1283397589791022,7.0201810347470595,5.624287386169439,-0.2663292879200034,-0.03662080416157129,-0.6457503383496215,1.2058962626018372,1.352'\n",
      "b'1.8444675088321243,0.5124621340420246,0.505783649224786,-0.20645711406004988,-0.021362018052499883,-0.05811312281214649,0.8332732875369839,-1.2658703497187516,4.513'\n",
      "b'1.6312258686346301,0.3522616607867429,0.04080576110152256,-0.1408895163348976,-0.4632103899987006,-0.06751623819156843,-0.8277122355407183,0.5966931783531273,3.376'\n",
      "b'-0.7543417158936074,-0.9293421252555106,-0.9212720434835953,0.1242806741969112,-0.5983960315181748,-0.18494335623235414,-0.8183808561975836,0.8513600414406984,1.717'\n",
      "b'-1.453851024367546,1.874166156711919,-1.1315714708271856,0.3611276016530489,-0.3978857847006997,-0.03273859332533962,-0.7390641317809511,0.646627857389904,1.875'\n"
     ]
    }
   ],
   "source": [
    "n_readers = 5 \n",
    "dataset = filename_dataset.interleave(\n",
    "    lambda filename: tf.data.TextLineDataset(filenames=filename).skip(1),\n",
    "    # tf.data.TextLineDataset(self, filenames, compression_type=None, buffer_size=None, num_parallel_reads=None)\n",
    "    # A Dataset : comprising lines from one or more text files.\n",
    "    # 由于第一行为特征名称，所以 skip（1）\n",
    "    cycle_length = n_readers\n",
    ")\n",
    "\n",
    "for line in dataset.take(10):\n",
    "    print(line.numpy())\n",
    "    # 获得标准化后的前 10 个训练集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "id": "G2aIwrmlIakQ",
    "outputId": "ecc05180-df38-4093-9b3b-a647c2507e82"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor: id=95, shape=(), dtype=int32, numpy=1>,\n",
      " <tf.Tensor: id=96, shape=(), dtype=int32, numpy=2>,\n",
      " <tf.Tensor: id=97, shape=(), dtype=float32, numpy=3.0>,\n",
      " <tf.Tensor: id=98, shape=(), dtype=string, numpy=b'4'>,\n",
      " <tf.Tensor: id=99, shape=(), dtype=float32, numpy=5.0>]\n"
     ]
    }
   ],
   "source": [
    "# ed\n",
    "\n",
    "# 使用 api 解析 csv\n",
    "# tf.io.decode_csv(str, record_defaults)\n",
    "# 例子\n",
    "sample_str = '1,2,3,4,5'  # 等价于 csv，即 -- 按行读取（一行），按列分隔（五列）的字符串\n",
    "record_defaults = [\n",
    "    tf.constant(0, dtype=tf.int32),\n",
    "    0,\n",
    "    np.nan,\n",
    "    \"hello\",\n",
    "    tf.constant([])\n",
    "]\n",
    "\n",
    "parsed_fields = tf.io.decode_csv(sample_str, record_defaults=record_defaults)\n",
    "# tf.io.decode_csv() --- Convert CSV records to tensors. Each column maps to one tensor.\n",
    "# record_defaults --- 指定生成的 tensor 的类型，上面用[]列表表明对不同列保存为 5 种不同类型\n",
    "pprint.pprint(parsed_fields)\n",
    "# 生成了 5 个 tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "colab_type": "code",
    "id": "yyiMH7yAKocE",
    "outputId": "1b400ca6-02d8-466d-ea95-fb24334ad0dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: id=111, shape=(8,), dtype=float32, numpy=\n",
      "array([-0.9868721 ,  0.8328631 , -0.18684709, -0.1488895 , -0.45323023,\n",
      "       -0.11504996,  1.6730974 , -0.74654967], dtype=float32)>,\n",
      " <tf.Tensor: id=112, shape=(1,), dtype=float32, numpy=array([1.138], dtype=float32)>)\n"
     ]
    }
   ],
   "source": [
    "def parse_csv_line(line, n_fields = 9):\n",
    "    record_defaults = [tf.constant(np.nan)] * n_fields\n",
    "    # np.nan -- 通常用于初始化的格式吗\n",
    "    parsed_fields = tf.io.decode_csv(line, record_defaults=record_defaults)\n",
    "    # 从传入的 csv 格式文件、字符串中获得 tensor\n",
    "\n",
    "    # 再把该 tensor 分开合并为 x、y 矩阵\n",
    "    x = tf.stack(parsed_fields[0:-1]) # 取第一至倒数第二列合并为一个 x 张量\n",
    "    y = tf.stack(parsed_fields[-1:])  # 取倒数第一列合并为一个 y 张量\n",
    "    #tf.stack() --  Stacks a list of tensors into one tensor.(按行或列合并张量)\n",
    "\n",
    "    return x, y\n",
    "\n",
    "# 例子\n",
    "pprint.pprint(parse_csv_line(b'-0.9868720801669367,0.832863080552588,-0.18684708416901633,-0.14888949288707784,-0.4532302419670616,-0.11504995754593579,1.6730974284189664,-0.7465496877362412,1.138',\n",
    "               n_fields=9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 259
    },
    "colab_type": "code",
    "id": "W7gwYkkoOB8B",
    "outputId": "20fdb4a7-8109-4ca2-e9e1-7af60834125a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<DatasetV1Adapter shapes: ((None, 8), (None, 1)), types: (tf.float32, tf.float32)>\n",
      "x:________________\n",
      "<tf.Tensor: id=196, shape=(3, 8), dtype=float32, numpy=\n",
      "array([[ 2.5150437 ,  1.0731637 ,  0.5574401 , -0.17273512, -0.6129126 ,\n",
      "        -0.01909157, -0.5710993 , -0.02749031],\n",
      "       [-0.097193  , -1.2497431 ,  0.36232963,  0.02690608,  1.0338118 ,\n",
      "         0.04588159,  1.3418335 , -1.635387  ],\n",
      "       [ 0.04326301, -1.0895426 , -0.38878718, -0.10789865, -0.68186635,\n",
      "        -0.0723871 , -0.8883662 ,  0.8213992 ]], dtype=float32)>\n",
      "y:________________\n",
      "<tf.Tensor: id=197, shape=(3, 1), dtype=float32, numpy=\n",
      "array([[5.00001],\n",
      "       [1.832  ],\n",
      "       [1.426  ]], dtype=float32)>\n"
     ]
    }
   ],
   "source": [
    "# 总过程\n",
    "# 1. filename -> dataset\n",
    "# 2. read file -> dataset -> datasets -> merge\n",
    "# 3. parse csv\n",
    "\n",
    "\n",
    "def csv_reader_dataset(filenames, n_readers=5, batch_size=32, n_parse_threads=5,\n",
    "                       shuffle_buffer_size=10000):\n",
    "    dataset = tf.data.Dataset.list_files(filenames)\n",
    "    dataset = dataset.repeat()\n",
    "    dataset = dataset.interleave(\n",
    "        lambda filename: tf.data.TextLineDataset(filename).skip(1),\n",
    "        cycle_length = n_readers\n",
    "    )\n",
    "    # 为了达到随机的效果（随机选择样本进行训练）：\n",
    "    dataset.shuffle(shuffle_buffer_size)\n",
    "    dataset = dataset.map(map_func=parse_csv_line, num_parallel_calls=n_parse_threads)\n",
    "    # .map() --- Maps map_func across the elements of this dataset.\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    # .batch() --- Combines consecutive elements of this dataset into batches.\n",
    "    return dataset\n",
    "\n",
    "\n",
    "train_set = csv_reader_dataset(train_filenames, batch_size=3)\n",
    "# batch_size=3 --- 一次从硬盘读取 3 个样本到内存，以供后续训练\n",
    "print(train_set)\n",
    "\n",
    "for x_batch, y_batch in train_set.take(1):\n",
    "    print(\"x:________________\")\n",
    "    pprint.pprint(x_batch)\n",
    "    print(\"y:________________\")\n",
    "    pprint.pprint(y_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vPh6EdEmVs2z"
   },
   "outputs": [],
   "source": [
    "# 读取三个集的 32 个样本到内存\n",
    "batch_size = 32\n",
    "train_set = csv_reader_dataset(train_filenames,\n",
    "                               batch_size = batch_size)\n",
    "valid_set = csv_reader_dataset(valid_filenames,\n",
    "                               batch_size = batch_size)\n",
    "test_set = csv_reader_dataset(test_filenames,\n",
    "                              batch_size = batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "IzEW72RvWeoE",
    "outputId": "fdf89ae8-25ff-475c-e155-0eabf4364874"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 348 steps, validate for 120 steps\n",
      "Epoch 1/100\n",
      "348/348 [==============================] - 2s 5ms/step - loss: 0.6293 - val_loss: 0.5036\n",
      "Epoch 2/100\n",
      "348/348 [==============================] - 1s 4ms/step - loss: 0.6920 - val_loss: 0.4754\n",
      "Epoch 3/100\n",
      "348/348 [==============================] - 1s 4ms/step - loss: 0.4577 - val_loss: 0.4384\n",
      "Epoch 4/100\n",
      "348/348 [==============================] - 1s 4ms/step - loss: 0.4227 - val_loss: 0.4293\n",
      "Epoch 5/100\n",
      "348/348 [==============================] - 1s 3ms/step - loss: 0.4892 - val_loss: 0.4234\n",
      "Epoch 6/100\n",
      "348/348 [==============================] - 1s 4ms/step - loss: 0.3925 - val_loss: 0.4035\n",
      "Epoch 7/100\n",
      "348/348 [==============================] - 1s 4ms/step - loss: 0.3879 - val_loss: 0.3975\n",
      "Epoch 8/100\n",
      "348/348 [==============================] - 1s 4ms/step - loss: 0.3749 - val_loss: 0.3936\n",
      "Epoch 9/100\n",
      "348/348 [==============================] - 1s 4ms/step - loss: 0.3745 - val_loss: 0.3839\n",
      "Epoch 10/100\n",
      "348/348 [==============================] - 1s 4ms/step - loss: 0.3580 - val_loss: 0.3748\n",
      "Epoch 11/100\n",
      "348/348 [==============================] - 1s 4ms/step - loss: 0.3564 - val_loss: 0.3757\n",
      "Epoch 12/100\n",
      "348/348 [==============================] - 1s 4ms/step - loss: 0.3732 - val_loss: 0.3691\n",
      "Epoch 13/100\n",
      "348/348 [==============================] - 1s 4ms/step - loss: 0.3433 - val_loss: 0.3650\n",
      "Epoch 14/100\n",
      "348/348 [==============================] - 1s 4ms/step - loss: 0.3574 - val_loss: 0.3637\n",
      "Epoch 15/100\n",
      "348/348 [==============================] - 1s 3ms/step - loss: 0.3411 - val_loss: 0.3563\n",
      "Epoch 16/100\n",
      "348/348 [==============================] - 1s 4ms/step - loss: 0.3314 - val_loss: 0.3539\n",
      "Epoch 17/100\n",
      "348/348 [==============================] - 1s 4ms/step - loss: 0.3526 - val_loss: 0.3549\n",
      "Epoch 18/100\n",
      "348/348 [==============================] - 1s 4ms/step - loss: 0.3356 - val_loss: 0.3536\n",
      "Epoch 19/100\n",
      "348/348 [==============================] - 1s 4ms/step - loss: 0.3403 - val_loss: 0.3436\n",
      "Epoch 20/100\n",
      "348/348 [==============================] - 1s 4ms/step - loss: 0.3297 - val_loss: 0.3503\n",
      "Epoch 21/100\n",
      "348/348 [==============================] - 1s 4ms/step - loss: 0.3336 - val_loss: 0.3388\n",
      "Epoch 22/100\n",
      "348/348 [==============================] - 1s 4ms/step - loss: 0.3238 - val_loss: 0.3427\n",
      "Epoch 23/100\n",
      "348/348 [==============================] - 1s 4ms/step - loss: 0.3252 - val_loss: 0.3425\n",
      "Epoch 24/100\n",
      "348/348 [==============================] - 1s 4ms/step - loss: 0.3277 - val_loss: 0.3399\n",
      "Epoch 25/100\n",
      "348/348 [==============================] - 1s 4ms/step - loss: 0.3239 - val_loss: 0.3367\n",
      "Epoch 26/100\n",
      "348/348 [==============================] - 1s 4ms/step - loss: 0.3218 - val_loss: 0.3440\n",
      "Epoch 27/100\n",
      "348/348 [==============================] - 1s 4ms/step - loss: 0.3218 - val_loss: 0.3340\n",
      "Epoch 28/100\n",
      "348/348 [==============================] - 1s 4ms/step - loss: 0.3116 - val_loss: 0.3316\n",
      "Epoch 29/100\n",
      "348/348 [==============================] - 1s 4ms/step - loss: 0.3270 - val_loss: 0.3318\n",
      "Epoch 30/100\n",
      "348/348 [==============================] - 1s 4ms/step - loss: 0.3104 - val_loss: 0.3406\n",
      "Epoch 31/100\n",
      "348/348 [==============================] - 1s 4ms/step - loss: 0.3270 - val_loss: 0.3286\n",
      "Epoch 32/100\n",
      "348/348 [==============================] - 1s 4ms/step - loss: 0.3065 - val_loss: 0.3360\n",
      "Epoch 33/100\n",
      "348/348 [==============================] - 1s 4ms/step - loss: 0.3182 - val_loss: 0.3280\n",
      "Epoch 34/100\n",
      "348/348 [==============================] - 1s 4ms/step - loss: 0.3098 - val_loss: 0.3273\n",
      "Epoch 35/100\n",
      "348/348 [==============================] - 1s 4ms/step - loss: 0.3121 - val_loss: 0.3314\n",
      "Epoch 36/100\n",
      "348/348 [==============================] - 1s 4ms/step - loss: 0.3156 - val_loss: 0.3321\n",
      "Epoch 37/100\n",
      "348/348 [==============================] - 1s 4ms/step - loss: 0.2999 - val_loss: 0.3270\n",
      "Epoch 38/100\n",
      "348/348 [==============================] - 1s 4ms/step - loss: 0.3179 - val_loss: 0.3397\n"
     ]
    }
   ],
   "source": [
    "# keras.models.Sequential() --- Linear stack of layers. 步骤化的图结构\n",
    "model = keras.models.Sequential([\n",
    "                                 keras.layers.Dense(30, activation='relu',input_shape=[8]),\n",
    "                                 keras.layers.Dense(8, activation='relu'),\n",
    "                                 keras.layers.Dense(1)\n",
    "])\n",
    "# 上面的图结构若只有全连接层，相当于线性回归了\n",
    "model.compile(loss=\"mean_squared_error\", optimizer=\"sgd\")\n",
    "callbacks = [keras.callbacks.EarlyStopping(patience=10, min_delta=1e-2)]\n",
    "\n",
    "history = model.fit(train_set,\n",
    "                    validation_data = valid_set,\n",
    "                    steps_per_epoch = 11160 // batch_size,\n",
    "                    validation_steps = 3870 // batch_size,\n",
    "                    epochs = 100,\n",
    "                    callbacks = callbacks\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "_b2SlslzZ9lW",
    "outputId": "dd79b7f7-ec56-4354-8b36-082fe8d5d347"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3447\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.3446803264643835"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_set, steps=5160 // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nCdjHUYpaNk4"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "4-03 tf_data_generate_csv.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
